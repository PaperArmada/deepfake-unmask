# deepfake-unmask
DSIR-0124 Capstone; Develop a neural net that can distinguish between computer generated and genuine portraits of people

### Problem Statement

Deepfakes and computer generated human likenesses have become surprisingly convincing, nearly to the point of being indistinguishable from reality to the untrained eye. Even with guidance, the average individual struggles to outperform chance when tasked with identifying generated images from real ones (Nightingale and Farid, 2022). Can a neural net be trained to identify real from generated images that outperforms human capability in this space?


### Summary



#### Data

Real images were collected from the Flickr Faces High Quality (FFHQ) database of over 70,000 high quality images

Generated images were harvested from the website this-person-does-not-exist.com, which generates new, random, high resolution styleGAN images aproximately every 3 seconds.

3500 images for training
400 images for validation
100 images for test

#### Methodology

##### Processing
1. Full resolution real and generated images were downconverted to FFHQ low resolution sizes (128 x 128 px) using a common interpolation method
2. All images preprocessed using VGG16 parameters, and separated into train, validation, and test batches

##### Modeling
Multiple model architectures were experimented with in an attempt to derive high accuracy values and minimize overfitting.
After some experimentation, desirable parameters were identified, but it was recognized that:

1. There was the potential for leaking a 'tell' into the generated dataset by way of inconsisted (at least unknown) interpolation methods between real and generated low resolution images (addressed by the common interpolation method mentioned above). And,
2. That the batching method was producing duplicate images in train/validation/test sets

After these issues were corrected, a final high performing model was generated using two convolutional layers with pooling, two fully connected layers with dropout, and a final fully connected output layer, over 30 epochs of training.

### Conclusions & Recommendations

#### Conclusions:
Although showing signs of overfitting during the final half of training, the model achieved an average accuracy of around 90% on validation and test data. This substantially outperforms human results, and affirms the original intention of this project.

Interesting, though still incompletely formed, insights were generated by creating feature map 'activation' values by summing up the total pixel values within a feature map, and then comparing the set of feature maps for different groups of images (high confidence real images, high confidence synthetic images, missclassified real images, etc.). High activation feature maps appear to point towards particularly important filters, and could be a useful tool in deriving insight or fighting overfitting (if there are too few feature maps that the model is depending on.).

#### Potential Future Steps:
1. There appears to be room to improve overall accuracy by addressing the overfitting seen in the model. The training loss curve was continuing a downward trend, with validation loss plateauing at accuracy levels around 90%. Model simplification and a larger number of epochs could improve performance.

2. The model is currently tailored towards a very specific type of input; it would be desirable to generalize the types of inputs the model could take in, away from the tightly defined cropping and alignment of the FFHQ

3. Continue exploration of feature map visualization and 'fingerprints'. While interesting and somewhat clarifying, the actual utility of the 'fingerprints' developed is unclear. Connecting these features to concrete aspects of the model or the data would be desirable, and finding additional ways to bridge the gap between this kind of metric and human intuitive visuals could be useful in guiding the design process of these types of neural nets, and perhaps reduce the need for random experimentation.
