{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e89942",
   "metadata": {},
   "source": [
    "# Deepfake Unmask - Data Collection\n",
    "## Matthew Reed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9519790",
   "metadata": {},
   "source": [
    "The intent of this project is to create a home-grown neural network that can outperform humans ([reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8872790/)) in identifying computer generated images of people.\n",
    "\n",
    "This notebook handles the collection of similar synthetic and real images, and their subsequent processing for use in a neural-net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e935a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e990c53",
   "metadata": {},
   "source": [
    "#### Establishing path to be scraped for synthetic images, and generating directory if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74de293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://this-person-does-not-exist.com'\n",
    "\n",
    "if os.path.isdir('./data/generated/High Res/') is False:\n",
    "    os.makedirs('./data/generated/High Res/')\n",
    "    \n",
    "path = './data/generated/High Res/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7587707",
   "metadata": {},
   "source": [
    "#### Demonstrate retrieval of full resolution images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5708496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 20.0% Completion: Retrieved 10 of targeted 50 images.\n",
      "Current Execution Time: 40.0 seconds. \n",
      "\n",
      "At 40.0% Completion: Retrieved 20 of targeted 50 images.\n",
      "Current Execution Time: 74.0 seconds. \n",
      "\n",
      "At 60.0% Completion: Retrieved 30 of targeted 50 images.\n",
      "Current Execution Time: 105.0 seconds. \n",
      "\n",
      "At 80.0% Completion: Retrieved 40 of targeted 50 images.\n",
      "Current Execution Time: 144.0 seconds. \n",
      "\n",
      "At 100.0% Completion: Retrieved 50 of targeted 50 images.\n",
      "Current Execution Time: 180.0 seconds. \n",
      "\n",
      "Final Results: Retrieved 50 of targeted 50 images.\n",
      "Current Execution Time: 180.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_0 = time.time()\n",
    "\n",
    "retrieve_n = 50\n",
    "retrieved = 0\n",
    "\n",
    "for image in range(retrieve_n):\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    good_status = res.status_code == 200\n",
    "    wait_cycle = 0\n",
    "    failed_connection = False\n",
    "    \n",
    "    while not good_status:\n",
    "        wait_cycle += 1\n",
    "        print('Site did not respond. Retrying...')\n",
    "        if wait_cycle > 5:\n",
    "            print(f'Could not reach site. Terminating request. \\\n",
    "            Loop collected {retrieved} out of {retrieve_n} image(s) prior to lost connection.')\n",
    "            failed_connection = True\n",
    "            break\n",
    "        time.sleep(3)\n",
    "        res = requests.get(url)\n",
    "        good_status = res.status_code == 200\n",
    "\n",
    "    if failed_connection:\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    image_json = soup.find('img', {'id': 'avatar'})\n",
    "    image_link = url + image_json['src']\n",
    "    image_name = re.findall(\"avatar\\S+.jpg\", image_link)[0]\n",
    "    image_path = path + '/' + image_name\n",
    "    image_res = requests.get(image_link)\n",
    "\n",
    "    if image_res.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    file = open(image_path, 'wb')\n",
    "    file.write(image_res.content)\n",
    "    file.close()\n",
    "    \n",
    "    retrieved += 1\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    if (image + 1) % (retrieve_n//5) == 0:\n",
    "        print(f'At {round(((image + 1) / retrieve_n) * 100, 1)}% Completion: Retrieved {retrieved} of targeted {retrieve_n} images.')\n",
    "        print(f'Current Execution Time: {round((time.time() - time_0), 0)} seconds. \\n')\n",
    "\n",
    "print(f'Final Results: Retrieved {retrieved} of targeted {retrieve_n} images.')\n",
    "print(f'Current Execution Time: {round((time.time() - time_0), 0)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8ad03",
   "metadata": {},
   "source": [
    "#### Selected single image from those harvested for inspection and exploration \n",
    "(if running for yourself, the path will need to be updated to a file in your directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30158d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread('./data/generated/High Res/avatar-008390911eb28a25b6dcbbea9cb607ac.jpg', cv2.IMREAD_UNCHANGED)\n",
    "img1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470d58c",
   "metadata": {},
   "source": [
    "#### Resizing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d670149",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 252\n",
    "height = 252\n",
    "dim = (width, height)\n",
    "resized = cv2.resize(img1, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40d2d",
   "metadata": {},
   "source": [
    "#### Image Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d74b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Image 1', resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da118c3f",
   "metadata": {},
   "source": [
    "#### Pulling in a fresh image from the website and resizing, then viewing the resulting image\n",
    "#### !!IMPORTANT!! Do not 'x' out of image popup; with the popup window selected, press any key to close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efcefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "image_json = soup.find('img', {'id': 'avatar'})\n",
    "image_link = url + image_json['src']\n",
    "image_name = re.findall(\"avatar\\S+.jpg\", image_link)[0]\n",
    "image_path = path + '/' + image_name\n",
    "image_res = requests.get(image_link, stream=True).raw\n",
    "\n",
    "image = np.asarray(bytearray(image_res.read()), dtype=\"uint8\")\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "width = 256\n",
    "height = 256\n",
    "dim = (width, height)\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "cv2.imshow('Image 1', resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2527883",
   "metadata": {},
   "source": [
    "#### See the difference in downscaling results based on interpolation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bb76590",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "good_status = res.status_code == 200\n",
    "wait_cycle = 0\n",
    "failed_connection = False\n",
    "\n",
    "while not good_status:\n",
    "    wait_cycle += 1\n",
    "    print('Site did not respond. Retrying...')\n",
    "    if wait_cycle > 5:\n",
    "        print(f'Could not reach site. Terminating request. \\\n",
    "        Loop collected {retrieved} out of {retrieve_n} image(s) prior to lost connection.')\n",
    "        failed_connection = True\n",
    "        break\n",
    "    time.sleep(3)\n",
    "    res = requests.get(url)\n",
    "    good_status = res.status_code == 200\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "image_json = soup.find('img', {'id': 'avatar'})\n",
    "image_link = url + image_json['src']\n",
    "image_name = re.findall(\"avatar\\S+.jpg\", image_link)[0]\n",
    "image_path = path + '/' + image_name\n",
    "image_res = requests.get(image_link, stream=True).raw\n",
    "\n",
    "#     if image_res.status_code != 200:\n",
    "#         continue\n",
    "\n",
    "image = np.asarray(bytearray(image_res.read()), dtype=\"uint8\")\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "width = 128\n",
    "height = 128\n",
    "dim = (width, height)\n",
    "resized = cv2.resize(image, dim, interpolation = cv2.INTER_LANCZOS4)\n",
    "\n",
    "\n",
    "resized_nearest = cv2.resize(image, dim, interpolation = cv2.INTER_NEAREST)\n",
    "resized_linear = cv2.resize(image, dim, interpolation = cv2.INTER_LINEAR)\n",
    "resized_area = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "resized_cubic = cv2.resize(image, dim, interpolation = cv2.INTER_CUBIC)\n",
    "resized_lanczos = cv2.resize(image, dim, interpolation = cv2.INTER_LANCZOS4)\n",
    "\n",
    "horizontal = np.concatenate((resized_nearest, resized_linear, resized_area, resized_cubic, resized_lanczos), axis=1)\n",
    "\n",
    "cv2.imshow('Image 1', horizontal)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # !!IMPORTANT!! Do not 'x' out of image popup; press any key to close. Will break jupyter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d1402",
   "metadata": {},
   "source": [
    "#### Function for quickly creating new paths without destroying existing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d5486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_path(path_string):\n",
    "    \n",
    "    if os.path.isdir(path_string) is False:\n",
    "        os.makedirs(path_string)\n",
    "\n",
    "    return path_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f37455",
   "metadata": {},
   "source": [
    "#### Low-res (128 x 128px) image collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7828252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_low_res(path, retrieve_n=500, interp='INTER_AREA'):\n",
    "    \n",
    "    time_0 = time.time()\n",
    "\n",
    "    path = path\n",
    "    retrieve_n = retrieve_n\n",
    "    retrieved = 0\n",
    "\n",
    "    for image in range(retrieve_n):\n",
    "\n",
    "        res = requests.get(url)\n",
    "        good_status = res.status_code == 200\n",
    "        wait_cycle = 0\n",
    "        failed_connection = False\n",
    "\n",
    "        while not good_status:\n",
    "            wait_cycle += 1\n",
    "            print('Site did not respond. Retrying...')\n",
    "            if wait_cycle > 5:\n",
    "                print(f'Could not reach site. Terminating request. \\\n",
    "                Loop collected {retrieved} out of {retrieve_n} image(s) prior to lost connection.')\n",
    "                failed_connection = True\n",
    "                break\n",
    "            time.sleep(3)\n",
    "            res = requests.get(url)\n",
    "            good_status = res.status_code == 200\n",
    "\n",
    "        if failed_connection:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "        image_json = soup.find('img', {'id': 'avatar'})\n",
    "        image_link = url + image_json['src']\n",
    "        image_name = re.findall(\"avatar\\S+.jpg\", image_link)[0]\n",
    "        image_path = path + '/' + image_name\n",
    "        image_res = requests.get(image_link, stream=True).raw\n",
    "\n",
    "    #     if image_res.status_code != 200:\n",
    "    #         continue\n",
    "\n",
    "        image = np.asarray(bytearray(image_res.read()), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if image.size == 0 or None:\n",
    "            print('Failed to collect image')\n",
    "            continue\n",
    "\n",
    "        width = 128\n",
    "        height = 128\n",
    "        dim = (width, height)\n",
    "        resized = cv2.resize(image, dim, interpolation = getattr(cv2, interp))\n",
    "\n",
    "        cv2.imwrite(image_path, resized)\n",
    "\n",
    "        time.sleep(3)\n",
    "        retrieved += 1\n",
    "\n",
    "    print(f'Retrieved {retrieved} of targeted {retrieve_n} images.')\n",
    "    print(f'Execution Time: {round((time.time() - time_0), 0)} seconds')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5830e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_low_res = generate_path('./data/generated/Low Res/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3d423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 301 of targeted 301 images.\n",
      "Execution Time: 1414.0 seconds\n"
     ]
    }
   ],
   "source": [
    "collect_low_res(path_low_res, retrieve_n=301, interp='INTER_AREA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e78bd9",
   "metadata": {},
   "source": [
    "## Experiment: Test whether preprocessing/interpolation of the synthetic images is 'giving away' the synthetics in the model\n",
    "\n",
    "The 128 x 128px data set from [Kaggle](https://www.kaggle.com/datasets/dullaz/flickrfaces-dataset-nvidia-128x128), while being very convenient and having my deep appreciation, does not specify how the full resolution images from the FFHQ database were downconverted. This poses a problem due to the fact that I apply a downconversion on the raw 1024 x 1024px synthetic images upon retrieval, and in turn must select an interpolation method to execute the downconversion. It is possible that the methods used for downconversion between real and synthetic images differ, and that there may be artifacts to either process that are present in the images purely due to preprocessing, and could be a factor in the neural net's ability to distinguish one from the other. Here, I bring in a small set of full resolution real images ([from here](https://drive.google.com/drive/folders/1tZUcXDBeOibC6jcMCtgRRz67pzrAHeHL)) and apply identical interpolation methods for the downconversion that I have applied to the raw synthetic images, so that a comparison model can be built and relative performance can be evaluated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd25c62",
   "metadata": {},
   "source": [
    "#### Downconvert full resolution real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb68901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale_imgs(full_res_dir, destination_dir, interp, width=128, height=128):\n",
    "    \n",
    "    destination_dir = destination_dir + interp + '/'\n",
    "    \n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.mkdir(destination_dir)\n",
    "\n",
    "    for image_name in os.listdir(full_res_dir):\n",
    "\n",
    "            image_path = full_res_dir + image_name\n",
    "            image = cv2.imread(image_path)\n",
    "                        \n",
    "            dim = (width, height)\n",
    "            resized = cv2.resize(image, dim, interpolation = getattr(cv2, interp))\n",
    "#             print((destination_dir + interp + '_' + image_name))\n",
    "            cv2.imwrite((destination_dir + interp + '_' + image_name), resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "214d5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res_dir = './data/real/FFHQ-high-res/69000/'\n",
    "destination_dir = './data/real/FFHQ-high-res/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4961e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_imgs(full_res_dir, destination_dir, 'INTER_AREA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "163d1ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_imgs(full_res_dir, destination_dir, 'INTER_LANCZOS4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a75b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res_dir = './data/real/FFHQ-high-res/68000/'\n",
    "destination_dir = './data/real/FFHQ-high-res/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dac73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscale_imgs(full_res_dir, destination_dir, 'INTER_AREA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
